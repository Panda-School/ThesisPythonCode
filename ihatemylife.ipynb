{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusion3Img2ImgPipeline\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np"
   ],
   "id": "5e195afe3dc1af58",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1b) Load CLIP for entailment testing\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ],
   "id": "9fee41da4b63aa0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Stable Diffusion img2img pipeline\n",
    "sd_pipe = StableDiffusion3Img2ImgPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3.5-medium\",\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
    ")\n",
    "sd_pipe.enable_model_cpu_offload()\n"
   ],
   "id": "7f25c5b371ebe755",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clip_entailment_score(image: Image.Image, sentence: str) -> float:\n",
    "    # 1) process image only\n",
    "    image_inputs = clip_processor(images=[image], return_tensors=\"pt\").to(device)\n",
    "    # 2) process text only\n",
    "    text_inputs  = clip_processor(text=[sentence], return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # pass only the pixel_values to get_image_features\n",
    "        image_embeds = clip_model.get_image_features(pixel_values=image_inputs.pixel_values)\n",
    "        # pass only the text tensors to get_text_features\n",
    "        text_embeds  = clip_model.get_text_features(\n",
    "            input_ids=text_inputs.input_ids,\n",
    "            attention_mask=text_inputs.attention_mask\n",
    "        )\n",
    "\n",
    "    # normalize\n",
    "    image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "    text_embeds  = text_embeds  / text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "    # cosine similarity\n",
    "    sim = (image_embeds * text_embeds).sum(dim=-1).item()\n",
    "    return sim\n",
    "\n",
    "def entails(image: Image.Image, sentence: str, threshold: float = 0.33) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if CLIP similarity â‰¥ threshold.\n",
    "    \"\"\"\n",
    "    score = clip_entailment_score(image, sentence)\n",
    "    return score >= threshold\n"
   ],
   "id": "c513f0a9b8f4273e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def edit_away(\n",
    "    image: Image.Image,\n",
    "    caption: str,\n",
    "    avoid_sentence: str,\n",
    "    strength: float = 0.75,\n",
    "    guidance_scale: float = 7.5,\n",
    "    num_inference_steps: int = 50\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Edit `image` to preserve `caption` but move it away from `avoid_sentence`.\n",
    "    Uses `avoid_sentence` as negative prompt.\n",
    "    \"\"\"\n",
    "    result = sd_pipe(\n",
    "        prompt=caption,\n",
    "        negative_prompt=avoid_sentence,\n",
    "        image=image,\n",
    "        strength=strength,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps\n",
    "    )\n",
    "    return result.images[0]"
   ],
   "id": "eacc0c296b816f0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "import json\n",
    "\n",
    "with open(\"./data.jsonl\", \"r\") as f:\n",
    "    data = [json.loads(line) for line in f]"
   ],
   "id": "a41892366da62098",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "caption_id: str = data[1]['captionID'].split(\"#\")[0]\n",
    "\n",
    "request = requests.get(\"https://hazeveld.org/snli-ve/images/\" + caption_id)\n",
    "\n",
    "img = Image.open(BytesIO(request.content)).convert(\"RGB\")"
   ],
   "id": "414b3dd046232725",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dict = {}\n",
    "for i in range(80):\n",
    "    caption_id: str = data[i]['captionID'].split(\"#\")[0]\n",
    "    request = requests.get(\"https://hazeveld.org/snli-ve/images/\" + caption_id)\n",
    "    image = Image.open(BytesIO(request.content)).convert(\"RGB\")\n",
    "    orig_score = entails(image, data[i]['sentence2'], threshold=0.23)\n",
    "    score2 = entails(image, data[i]['sentence1'], threshold=0.23)\n",
    "    dict[caption_id] = [orig_score, score2]"
   ],
   "id": "88b77990aeab8f17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "orig_score = entails(image, data[1]['sentence2'], threshold=0.20)",
   "id": "f1fe0b1ca17d1227",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image = edit_away(\n",
    "    img,\n",
    "    caption=data[1]['sentence1'],\n",
    "    avoid_sentence=data[1]['sentence2'],\n",
    ")"
   ],
   "id": "4e2380f9f685cf97",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
